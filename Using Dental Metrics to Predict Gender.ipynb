{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de25ad13",
   "metadata": {},
   "source": [
    "# Step 1: Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8366502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf071f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"Display.max_rows\", None)\n",
    "pd.set_option(\"Display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44f308",
   "metadata": {},
   "source": [
    "# Step 2 : Import Dataset using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dentistry Dataset.csv\")\n",
    "df.head()                                # To check the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the shape of the DataFrame\n",
    "print(f\"Total number of rows : {df.shape[0]}\")\n",
    "print(f\"Total number of columns : {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the datatypes of each feature\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bffd0d",
   "metadata": {},
   "source": [
    "# Step 3 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the null values columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the Gender column\n",
    "gender_dummies = pd.get_dummies(df['Gender'], prefix='Gender')\n",
    "\n",
    "# Concatenate the original DataFrame with the dummy variables\n",
    "df_numeric_only = pd.concat([df, gender_dummies], axis=1)\n",
    "\n",
    "# Removing the unnecessary features\n",
    "df_numeric_only = df_numeric_only.drop(columns=[\"Sample ID\",\"Sl No\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d899dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40023b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split independent & dependent variable i.e X and Y\n",
    "X = df_numeric_only.drop([\"Gender\"],axis=1)    # independent feature should be DataFrame or 2-dimensional array\n",
    "y = df_numeric_only[\"Gender\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e55b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the X variable\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# dataframe\n",
    "x = df_numeric_only.drop(\"Gender\", axis=1)\n",
    "normalizer = Normalizer()\n",
    "x_normalized = normalizer.fit_transform(x)\n",
    "print(x_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca710219",
   "metadata": {},
   "source": [
    "# Summary Statistics of Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2a0fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_numeric_only.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915115cc",
   "metadata": {},
   "source": [
    "# Step 4 : Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39dee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9eb5635",
   "metadata": {},
   "source": [
    "# Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1944818",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_numeric_only.corr(numeric_only=True)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c13e8",
   "metadata": {},
   "source": [
    "# Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(df_numeric_only.corr(numeric_only=True), annot=True , cmap='coolwarm' )\n",
    "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "plt.xticks(rotation=80)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7957e",
   "metadata": {},
   "source": [
    "As you can see from this correlation heat map, that their are some features that are highly correlated with others features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e15ff",
   "metadata": {},
   "source": [
    "# Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "# To create countplot\n",
    "sns.countplot(df_numeric_only, x=\"Gender\", palette='Set2')\n",
    "plt.title(\"Count of Gender\")\n",
    "plt.ylabel(\"Count\", labelpad=20, fontsize=10)\n",
    "plt.xticks(rotation=45)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "    ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89a4f4",
   "metadata": {},
   "source": [
    "As you can see here their are equal number of males & females"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156474d",
   "metadata": {},
   "source": [
    "# Pie Chart representation to determine the percentage of outliers in each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065501b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary python library\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create a dropdown widget for selecting a single column\n",
    "column_selector = widgets.Dropdown(\n",
    "    options=df_numeric_only.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist(),\n",
    "    description='Column',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Function to plot pie chart for the selected column\n",
    "def plot_outliers(selected_column):\n",
    "    Q1 = df_numeric_only[selected_column].quantile(0.25)\n",
    "    Q3 = df_numeric_only[selected_column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = (df_numeric_only[selected_column] < Q1 - 1.5 * IQR) | (df_numeric_only[selected_column] > Q3 + 1.5 * IQR)\n",
    "\n",
    "    num_outliers = outliers.sum()\n",
    "    num_non_outliers = len(df_numeric_only[selected_column]) - num_outliers\n",
    "\n",
    "    sizes = [num_outliers, num_non_outliers]\n",
    "    labels = [\"Outliers\", \"Non-Outliers\"]\n",
    "    colors = [\"#ff9999\", \"#66b3ff\"]\n",
    "    explode = (0.1, 0)\n",
    "\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Display the dropdown widget again\n",
    "    display(column_selector)\n",
    "    \n",
    "    # Plot the new pie chart\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "    ax.axis(\"equal\")\n",
    "    plt.title(\"Percentage of outliers in {}\".format(selected_column))\n",
    "    plt.show()\n",
    "\n",
    "# Function to handle the interaction\n",
    "def on_column_select(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        selected_column = change['new']\n",
    "        plot_outliers(selected_column)\n",
    "\n",
    "# Attach the handler to the dropdown widget\n",
    "column_selector.observe(on_column_select)\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(column_selector)\n",
    "\n",
    "# Initial plot\n",
    "plot_outliers(column_selector.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc608f",
   "metadata": {},
   "source": [
    "# Boxplot of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df_numeric_only.select_dtypes(include=[\"float64\",\"int64\"])\n",
    "\n",
    "# To determine the subplots and figure size\n",
    "fig, axes = plt.subplots(6,2, figsize=(12,15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# To create a Boxplot for all the numerical features.\n",
    "for i, col in enumerate(numerical_cols.columns):\n",
    "    sns.boxplot(x=numerical_cols[col], ax=axes[i], color=\"violet\")\n",
    "    axes[i].set_title(col)\n",
    "    \n",
    "# Hide any remaining empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9c2c0",
   "metadata": {},
   "source": [
    "# Histogram plot for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df_numeric_only.select_dtypes(include=[\"float64\",\"int64\"])\n",
    "plt.figure(figsize=(15,15))\n",
    "# To create a histogram plot for all the numerical features\n",
    "for count,i in enumerate(numerical_cols):\n",
    "    plt.subplot(7,2,count+1)\n",
    "    sns.histplot(numerical_cols, x=i, kde=True, stat='density', color='violet')\n",
    "    plt.xlabel(i, fontsize=20)\n",
    "    plt.ylabel('Density', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa9578",
   "metadata": {},
   "source": [
    "From this histogram plot we can see the data distribution and skewness for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df_numeric_only, x=\"right canine width intraoral\", y=\"right canine index casts\", color='violet')\n",
    "plt.title(\"Relationship between right canine width intraoral vs right canine index casts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709ad8a",
   "metadata": {},
   "source": [
    "The figure shows a positive correlation between the width of the right canine tooth measured intraorally and the index of the right canine tooth measured on casts. This means that as the width of the right canine tooth measured intraorally increases, the index of the right canine tooth measured on casts also tends to increase. This correlation is not perfect, but it is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ade1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df_numeric_only, x=\"right canine width intraoral\", y=\"left canine width intraoral\", color='violet')\n",
    "plt.title(\"Relationship between right canine width intraoral vs left canine width intraoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f62b8d",
   "metadata": {},
   "source": [
    "This above figure shows a positive correlation between each other, if one feature increases than the other one also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe22135",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df_numeric_only, x=\"left canine width intraoral\", y=\"left canine width casts\", color='r')\n",
    "plt.title(\"Relationship between Left canine width intraoral vs Left canine width casts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737bc77",
   "metadata": {},
   "source": [
    "It shows a positive correlation between left canine width casts and left canine width intraoral, it is basically directly proportional to each other, as one increases other one also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8faaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df_numeric_only, x=\"right canine index casts\", y=\"left canine index casts\", color='r')\n",
    "plt.title(\"Relationship between right canine index casts vs Left canine index casts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74524261",
   "metadata": {},
   "source": [
    "The scatter plot shows a positive correlation between the right and left canine index casts. This means that as the right canine index cast increases, the left canine index cast also tends to increase. The strong positive correlation suggests that the measurements of the right and left canine index casts are highly related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb32099",
   "metadata": {},
   "source": [
    "# Step 5 : Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbbb8d",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65533905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split independent & dependent variable i.e X and Y\n",
    "X = df_numeric_only.drop([\"Gender\"],axis=1)    # independent feature should be DataFrame or 2-dimensional array\n",
    "y = df_numeric_only[\"Gender\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96720113",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# To check the train and test score\n",
    "train_score = log_reg.score(X_train,y_train)\n",
    "test_score = log_reg.score(X_test,y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Train score : {}\".format(train_score))\n",
    "print(\"Test score : {}\".format(test_score))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc830d6f",
   "metadata": {},
   "source": [
    "# Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c32aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "dec_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = dec_tree.predict(X_test)\n",
    "\n",
    "# To check the train and test score\n",
    "train_score = dec_tree.score(X_train,y_train)\n",
    "test_score = dec_tree.score(X_test,y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Train score : {}\".format(train_score))\n",
    "print(\"Test score : {}\".format(test_score))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafc605",
   "metadata": {},
   "source": [
    "# Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4dae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# To check the train and test score\n",
    "train_score = rfc.score(X_train,y_train)\n",
    "test_score = rfc.score(X_test,y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Train score : {}\".format(train_score))\n",
    "print(\"Test score : {}\".format(test_score))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56316562",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=500, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c230e0",
   "metadata": {},
   "source": [
    "# XGBOOST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d387ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a023ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert categorical labels to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create a DMatrix object for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_encoded)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_encoded)\n",
    "\n",
    "# Define the XGBoost classifier parameters\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,  # number of classes\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=1000)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "train_eval = bst.eval(dtrain)\n",
    "print(\"Train eval:\", train_eval)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_eval = bst.eval(dtest)\n",
    "print(\"Test eval:\", test_eval)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "# Convert y_pred back to original labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9f650",
   "metadata": {},
   "source": [
    "# Conclusion :- All the models give good score with high accuracy, so our training data is best fit with all the models(logisticRegression, Decision Tree classifier, Random forest classifier, XGboost classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
